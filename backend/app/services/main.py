from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
import json
import time
import asyncio
from datetime import datetime
from typing import Dict, Any
import uvicorn
import logging
from qwen_model import qwen_model
from vector_store import medical_vector_store, initialize_sample_data
from graph_rag import medical_graph_rag, initialize_graph_knowledge
from langgraph_agent import medical_agent

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="ì„œìš¸ì•„ì‚°ë³‘ì› AI ì˜ë£Œ í”Œë«í¼",
    description="LangChain ê¸°ë°˜ ì˜ë£Œ AI ë°ì´í„° ë¶„ì„ í”Œë«í¼",
    version="1.0.0"
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],  # React ê°œë°œ ì„œë²„
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì„¸ì…˜ ìƒíƒœ ì €ì¥ì†Œ (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” Redis ì‚¬ìš©)
sessions = {}

# ëª¨ë¸ ë¡œë“œ ìƒíƒœ
model_loaded = False

@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ì‹œ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ëª¨ë¸ ë¡œë“œ"""
    logger.info("Server starting up...")
    # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ëª¨ë¸ ë¡œë“œ (ì„œë²„ ì‹œì‘ì„ ë¸”ë¡œí‚¹í•˜ì§€ ì•ŠìŒ)
    asyncio.create_task(load_model_background())

async def load_model_background():
    """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ëª¨ë¸ ë° ì§€ì‹ë² ì´ìŠ¤ ë¡œë“œ"""
    global model_loaded
    logger.info("Starting to load Qwen model and knowledge bases in background...")
    try:
        await asyncio.sleep(1)  # ì„œë²„ê°€ ë¨¼ì € ì‹œì‘ë˜ë„ë¡ ì ì‹œ ëŒ€ê¸°
        
        # Qwen ëª¨ë¸ ë¡œë“œ
        success = qwen_model.load_model()
        if success:
            model_loaded = True
            logger.info("Qwen model loaded successfully")
        else:
            logger.error("Failed to load Qwen model")
        
        # ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”
        logger.info("Initializing vector store...")
        initialize_sample_data()
        
        # GraphRAG ì´ˆê¸°í™”
        logger.info("Initializing GraphRAG knowledge base...")
        initialize_graph_knowledge()
        
        logger.info("All background initialization completed")
        
    except Exception as e:
        logger.error(f"Error in background initialization: {e}")

@app.get("/")
async def root():
    return {"message": "ì„œìš¸ì•„ì‚°ë³‘ì› AI ì˜ë£Œ í”Œë«í¼ API"}

@app.get("/api/v1/streaming/session/{session_id}/status")
async def get_session_status(session_id: str):
    """ì„¸ì…˜ ìƒíƒœ í™•ì¸"""
    return {
        "status": "connected", 
        "session_id": session_id,
        "model_loaded": model_loaded,
        "model_name": "Qwen/Qwen3-8B" if model_loaded else "Mock Model"
    }

async def generate_medical_response(query: str, session_id: str, user_type: str):
    """ì˜ë£Œ AI ì‘ë‹µ ìƒì„± (ìŠ¤íŠ¸ë¦¬ë°) - GraphRAG ë° ë²¡í„° ê²€ìƒ‰ í†µí•©"""
    
    # ì„¸ì…˜ ì‹œì‘ ì´ë²¤íŠ¸
    yield f"data: {json.dumps({'event_type': 'session_start', 'data': {'session_id': session_id}, 'timestamp': datetime.now().isoformat(), 'session_id': session_id})}\n\n"
    await asyncio.sleep(0.5)
    
    # GraphRAG ê²€ìƒ‰ ìˆ˜í–‰
    yield f"data: {json.dumps({'event_type': 'step_update', 'data': {'step': 'GraphRAG ì§€ì‹ ê·¸ë˜í”„ ê²€ìƒ‰ ì¤‘...'}, 'timestamp': datetime.now().isoformat(), 'session_id': session_id})}\n\n"
    await asyncio.sleep(0.3)
    
    try:
        # GraphRAG ê²€ìƒ‰
        graph_results = medical_graph_rag.graphrag_search(query, user_type=user_type, max_results=3)
        graph_context = graph_results.get('graph_context', '')
        
        # ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰
        yield f"data: {json.dumps({'event_type': 'step_update', 'data': {'step': 'ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ ì¤‘...'}, 'timestamp': datetime.now().isoformat(), 'session_id': session_id})}\n\n"
        await asyncio.sleep(0.3)
        
        vector_results = medical_vector_store.search_by_medical_context(query, user_type=user_type)
        vector_context = ""
        if vector_results:
            vector_context = "\\n\\n".join([
                f"**{result['metadata'].get('title', 'No title')}**: {result['content'][:200]}..."
                for result in vector_results[:3]
            ])
        
        # í†µí•© ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        combined_context = ""
        if graph_context:
            combined_context += f"**ê·¸ë˜í”„ ì§€ì‹:**\\n{graph_context}\\n\\n"
        if vector_context:
            combined_context += f"**ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼:**\\n{vector_context}\\n\\n"
        
        # ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸ ì´ë²¤íŠ¸ (ê²€ìƒ‰ ê²°ê³¼ í¬í•¨)
        memory_context = {
            "previous_symptoms": ["ë‘í†µ", "ë°œì—´"] if "ì¦ìƒ" in query else [],
            "medication_history": ["íƒ€ì´ë ˆë†€"] if "ì•½" in query else [],
            "message_count": 1,
            "graph_entities": len(graph_results.get('graph_results', [])),
            "vector_docs": len(vector_results),
            "knowledge_context": combined_context[:500] + "..." if len(combined_context) > 500 else combined_context
        }
        yield f"data: {json.dumps({'event_type': 'memory_context', 'data': memory_context, 'timestamp': datetime.now().isoformat(), 'session_id': session_id})}\n\n"
        await asyncio.sleep(0.3)
        
    except Exception as e:
        logger.error(f"Knowledge base search error: {e}")
        # ê¸°ë³¸ ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸
        memory_context = {
            "previous_symptoms": ["ë‘í†µ", "ë°œì—´"] if "ì¦ìƒ" in query else [],
            "medication_history": ["íƒ€ì´ë ˆë†€"] if "ì•½" in query else [],
            "message_count": 1,
            "search_error": str(e)
        }
        yield f"data: {json.dumps({'event_type': 'memory_context', 'data': memory_context, 'timestamp': datetime.now().isoformat(), 'session_id': session_id})}\n\n"
        await asyncio.sleep(0.3)
    
    # ì²˜ë¦¬ ë‹¨ê³„ë³„ ì—…ë°ì´íŠ¸
    if model_loaded:
        steps = [
            "ì‚¬ìš©ì ì§ˆì˜ ë¶„ì„ ì¤‘...",
            "Qwen 3 8B ëª¨ë¸ ë¡œë”© ì¤‘...",
            "ì˜ë£Œ ì»¨í…ìŠ¤íŠ¸ ì ìš© ì¤‘...",
            "AI ì‘ë‹µ ìƒì„± ì¤‘..."
        ]
    else:
        steps = [
            "ì‚¬ìš©ì ì§ˆì˜ ë¶„ì„ ì¤‘...",
            "ëª¨ì˜ ì‘ë‹µ ëª¨ë“œë¡œ ì²˜ë¦¬ ì¤‘...",
            "ì‘ë‹µ ìƒì„± ì¤‘..."
        ]
    
    for i, step in enumerate(steps):
        yield f"data: {json.dumps({'event_type': 'step_update', 'data': {'step': step}, 'timestamp': datetime.now().isoformat(), 'session_id': session_id})}\n\n"
        await asyncio.sleep(0.5)
    
    # LangGraph ì—ì´ì „íŠ¸ ì‚¬ìš© ë˜ëŠ” ê¸°ë³¸ ì‘ë‹µ
    if model_loaded:
        logger.info(f"Generating response with LangGraph agent for query: {query}")
        try:
            # LangGraph ì—ì´ì „íŠ¸ë¡œ ì²˜ë¦¬
            yield f"data: {json.dumps({'event_type': 'step_update', 'data': {'step': 'LangGraph ì—ì´ì „íŠ¸ ì²˜ë¦¬ ì¤‘...'}, 'timestamp': datetime.now().isoformat(), 'session_id': session_id})}\n\n"
            await asyncio.sleep(0.3)
            
            agent_result = await medical_agent.process_query(
                query=query,
                user_type=user_type,
                session_id=session_id
            )
            
            # ì—ì´ì „íŠ¸ ì²˜ë¦¬ ê³¼ì •ì„ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì „ì†¡
            if agent_result.get('tools_used'):
                yield f"data: {json.dumps({'event_type': 'step_update', 'data': {'step': f'ë„êµ¬ ì‚¬ìš© ì™„ë£Œ: {', '.join(agent_result['tools_used'])}'}, 'timestamp': datetime.now().isoformat(), 'session_id': session_id})}\n\n"
                await asyncio.sleep(0.2)
            
            # ì‘ë‹µì„ í† í° ë‹¨ìœ„ë¡œ ìŠ¤íŠ¸ë¦¬ë°
            response_text = agent_result.get('response', 'ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')
            words = response_text.split()
            for word in words:
                yield f"data: {json.dumps({'event_type': 'token', 'data': {'content': word + ' '}, 'timestamp': datetime.now().isoformat(), 'session_id': session_id})}\n\n"
                await asyncio.sleep(0.05)
                
        except Exception as e:
            logger.error(f"Error with LangGraph agent: {e}")
            error_msg = f"LangGraph ì—ì´ì „íŠ¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            yield f"data: {json.dumps({'event_type': 'token', 'data': {'content': error_msg}, 'timestamp': datetime.now().isoformat(), 'session_id': session_id})}\n\n"
    else:
        # ëª¨ì˜ ì‘ë‹µ (ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì€ ê²½ìš°)
        logger.info("Using mock response - Qwen model not loaded")
        
        if user_type == "doctor":
            response_text = f"""ì•ˆë…•í•˜ì„¸ìš”. ì˜ë£Œì§„ë‹˜ì˜ ì§ˆì˜ '{query}'ì— ëŒ€í•œ ì„ìƒì  ë¶„ì„ì„ ì œê³µë“œë¦½ë‹ˆë‹¤.

ğŸ¥ **ì„ìƒ ì§„ë‹¨ ì§€ì›**
- í™˜ìì˜ ì¦ìƒì„ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•œ ê²°ê³¼
- ê°ë³„ì§„ë‹¨ í•­ëª©ë“¤ì„ ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ì •ë¦¬
- ì¶”ê°€ ê²€ì‚¬ê°€ í•„ìš”í•œ í•­ëª©ë“¤ ì œì•ˆ

ğŸ“Š **ë°ì´í„° ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸**
- ìœ ì‚¬ ì¼€ì´ìŠ¤ ë¶„ì„ ê²°ê³¼
- ì¹˜ë£Œ íš¨ê³¼ ì˜ˆì¸¡ ëª¨ë¸
- ì•½ë¬¼ ìƒí˜¸ì‘ìš© ì²´í¬

âš ï¸ *í˜„ì¬ ëª¨ì˜ ì‘ë‹µ ëª¨ë“œì…ë‹ˆë‹¤. ì‹¤ì œ Qwen 3 8B ëª¨ë¸ ë¡œë”©ì´ í•„ìš”í•©ë‹ˆë‹¤.*"""
        
        elif user_type == "researcher":
            response_text = f"""ì—°êµ¬ìë‹˜ì˜ ì§ˆì˜ '{query}'ì— ëŒ€í•œ ì—°êµ¬ ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤.

ğŸ”¬ **ì—°êµ¬ ë°ì´í„° ë¶„ì„**
- ê´€ë ¨ ì˜ë£Œ ë…¼ë¬¸ ë° ì—°êµ¬ ë™í–¥
- í†µê³„ì  ë¶„ì„ ê²°ê³¼ ë° p-value
- ì½”í˜¸íŠ¸ ì—°êµ¬ ë°ì´í„° ë¹„êµ

ğŸ“ˆ **ì—°êµ¬ ì¸ì‚¬ì´íŠ¸**
- ìµœì‹  ì„ìƒì‹œí—˜ ê²°ê³¼
- ë©”íƒ€ë¶„ì„ ë°ì´í„°
- ì—°êµ¬ ë°©ë²•ë¡  ì œì•ˆ

âš ï¸ *í˜„ì¬ ëª¨ì˜ ì‘ë‹µ ëª¨ë“œì…ë‹ˆë‹¤. ì‹¤ì œ Qwen 3 8B ëª¨ë¸ ë¡œë”©ì´ í•„ìš”í•©ë‹ˆë‹¤.*"""
        
        else:  # patient
            response_text = f"""ì•ˆë…•í•˜ì„¸ìš”! í™˜ìë¶„ì˜ ì§ˆë¬¸ '{query}'ì— ëŒ€í•´ ì¹œì ˆí•˜ê²Œ ë‹µë³€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.

ğŸ’Š **ê±´ê°• ì •ë³´ ì•ˆë‚´**
- ì¦ìƒì— ëŒ€í•œ ì¼ë°˜ì ì¸ ì„¤ëª…
- ìƒí™œ ì† ê´€ë¦¬ ë°©ë²•
- ì–¸ì œ ë³‘ì›ì„ ë°©ë¬¸í•´ì•¼ í•˜ëŠ”ì§€

âš•ï¸ **ì£¼ì˜ì‚¬í•­**
- ì´ ì •ë³´ëŠ” ì˜í•™ì  ì¡°ì–¸ì„ ëŒ€ì²´í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤
- ì‹¬ê°í•œ ì¦ìƒì´ ìˆë‹¤ë©´ ì¦‰ì‹œ ì˜ë£Œì§„ê³¼ ìƒë‹´í•˜ì„¸ìš”
- ì •í™•í•œ ì§„ë‹¨ì€ ì „ë¬¸ì˜ì˜ ì§„ë£Œê°€ í•„ìš”í•©ë‹ˆë‹¤

âš ï¸ *í˜„ì¬ ëª¨ì˜ ì‘ë‹µ ëª¨ë“œì…ë‹ˆë‹¤. ì‹¤ì œ Qwen 3 8B ëª¨ë¸ ë¡œë”©ì´ í•„ìš”í•©ë‹ˆë‹¤.*"""
        
        # ì‘ë‹µ í…ìŠ¤íŠ¸ë¥¼ í† í° ë‹¨ìœ„ë¡œ ìŠ¤íŠ¸ë¦¬ë°
        words = response_text.split()
        for word in words:
            yield f"data: {json.dumps({'event_type': 'token', 'data': {'content': word + ' '}, 'timestamp': datetime.now().isoformat(), 'session_id': session_id})}\n\n"
            await asyncio.sleep(0.1)
    
    # ì™„ë£Œ ì´ë²¤íŠ¸
    final_memory = memory_context.copy()
    final_memory["last_query"] = query
    final_memory["message_count"] += 1
    
    yield f"data: {json.dumps({'event_type': 'completion', 'data': {'final_memory': final_memory}, 'timestamp': datetime.now().isoformat(), 'session_id': session_id})}\n\n"

@app.post("/api/v1/streaming/medical-query")
async def stream_medical_query(request: Dict[Any, Any]):
    """ì˜ë£Œ ì§ˆì˜ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬"""
    query = request.get("query", "")
    session_id = request.get("session_id", f"session_{int(time.time())}")
    user_type = request.get("user_type", "patient")
    
    if not query:
        raise HTTPException(status_code=400, detail="Query is required")
    
    return StreamingResponse(
        generate_medical_response(query, session_id, user_type),
        media_type="text/plain",
        headers={"Cache-Control": "no-cache", "Connection": "keep-alive"}
    )

@app.get("/api/v1/streaming/test-stream")
async def test_stream():
    """ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸"""
    async def generate():
        for i in range(5):
            yield f"data: í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€ {i+1}\n\n"
            await asyncio.sleep(1)
    
    return StreamingResponse(generate(), media_type="text/plain")

@app.get("/api/v1/knowledge/vector-stats")
async def get_vector_stats():
    """ë²¡í„° ìŠ¤í† ì–´ í†µê³„"""
    try:
        stats = medical_vector_store.get_collection_stats()
        return {
            "status": "success",
            "vector_store_stats": stats,
            "total_documents": sum(stats.values())
        }
    except Exception as e:
        return {
            "status": "error", 
            "error": str(e),
            "vector_store_stats": {}
        }

@app.get("/api/v1/knowledge/graph-stats")
async def get_graph_stats():
    """ê·¸ë˜í”„ ìŠ¤í† ì–´ í†µê³„"""
    try:
        stats = medical_graph_rag.get_graph_statistics()
        return {
            "status": "success",
            "graph_stats": stats
        }
    except Exception as e:
        return {
            "status": "error",
            "error": str(e), 
            "graph_stats": {}
        }

@app.post("/api/v1/knowledge/vector-search")
async def vector_search(request: Dict[Any, Any]):
    """ë²¡í„° ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"""
    try:
        query = request.get("query", "")
        user_type = request.get("user_type", "patient")
        
        if not query:
            raise HTTPException(status_code=400, detail="Query is required")
        
        results = medical_vector_store.search_by_medical_context(query, user_type=user_type)
        
        return {
            "status": "success",
            "query": query,
            "user_type": user_type,
            "num_results": len(results),
            "results": results
        }
        
    except Exception as e:
        return {
            "status": "error",
            "error": str(e),
            "results": []
        }

@app.post("/api/v1/knowledge/graph-search")  
async def graph_search(request: Dict[Any, Any]):
    """GraphRAG ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"""
    try:
        query = request.get("query", "")
        user_type = request.get("user_type", "patient")
        
        if not query:
            raise HTTPException(status_code=400, detail="Query is required")
        
        results = medical_graph_rag.graphrag_search(query, user_type=user_type)
        
        return {
            "status": "success",
            "query": query,
            "user_type": user_type,
            "graph_results": results
        }
        
    except Exception as e:
        return {
            "status": "error", 
            "error": str(e),
            "graph_results": {}
        }

@app.post("/api/v1/agent/query")
async def agent_query(request: Dict[Any, Any]):
    """LangGraph ì—ì´ì „íŠ¸ ì§ˆì˜ ì²˜ë¦¬"""
    try:
        query = request.get("query", "")
        user_type = request.get("user_type", "patient")
        session_id = request.get("session_id", f"session_{int(time.time())}")
        
        if not query:
            raise HTTPException(status_code=400, detail="Query is required")
        
        result = await medical_agent.process_query(
            query=query,
            user_type=user_type,
            session_id=session_id
        )
        
        return {
            "status": "success",
            "query": query,
            "user_type": user_type,
            "session_id": session_id,
            "agent_result": result
        }
        
    except Exception as e:
        return {
            "status": "error",
            "error": str(e),
            "agent_result": {}
        }

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000, reload=True)